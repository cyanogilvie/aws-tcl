if {[lsearch [namespace children] ::tcltest] == -1} {
    package require tcltest
    namespace import ::tcltest::*
}

tcltest::loadTestedCommands
package require aws 2
package require rl_json
package require rltest
interp alias {} json {} ::rl_json::json

tcltest::testConstraint have_s3_bucket [try { #<<<
	aws::helpers::get_creds
	if {[info exists ::env(BUCKET)]} {
		set bucket			$::env(BUCKET)
		set bucketlocation	[aws s3 get_bucket_location -bucket $bucket]
		set bucket_region	[json get -default {} $bucketlocation LocationConstraint]
	} else {
		set buckets_resp	[aws s3 list_buckets]
		if {[json exists $buckets_resp Buckets 0]} {
			json foreach b [json extract $buckets_resp Buckets] {
				if {[json get $b Name] eq "cyan-archive"} continue
				set bucket	[json get $b Name]
				break
			}
		}
		#if {[json exists $buckets_resp Buckets 0 Name]} {
		#	set bucket	[json get $buckets_resp Buckets 0 Name]
		#}
		set bucketlocation	[aws s3 get_bucket_location -bucket $bucket]
		set bucket_region	[json get -default {} $bucketlocation LocationConstraint]
	}
	if {$bucket_region eq ""} {set bucket_region us-east-1}
	#puts stderr "bucket: ($bucket), bucket_region: ($bucket_region), bucketlocation: ($bucketlocation)"
} trap {AWS NO_CREDENTIALS} {} {
	return -level 0 false
} on ok {} {
	info exists bucket
} on error {errmsg options} {
	puts stderr "Error checking for AWS credentials: ([dict get $options -errorcode]) [dict get $options -errorinfo]"
	return -level 0 false
} finally {
	unset -nocomplain buckets_resp errmsg options b
}]
#>>>

test s3sigv4-1.1 {get key, no encoding} -constraints have_s3_bucket -body { #<<<
	aws s3 get_object -region $bucket_region -bucket $bucket -key foo
} -match glob -returnCodes error -errorCode {AWS NoSuchKey *} -result {AWS: The specified key does not exist.}
#>>>
test s3sigv4-1.2 {get key, no encoding} -constraints have_s3_bucket -body { #<<<
	#aws s3 get_object -region aws-global -use_global_endpoint 1 -bucket $bucket -key foo\u306f
	list [rltest compare_json [aws s3 get_object -region $bucket_region -bucket $bucket -key foo\u306f -payload payload] [json template {
		{
			"AcceptRanges":			"bytes",
			"LastModified":			"?G:*",
			"ContentLength":		3,
			"ETag":					"?G:*",
			"ContentType":			"binary/octet-stream",
			"ServerSideEncryption":	"AES256",
			"Metadata":				{}
		}
	}]] $payload
} -cleanup {
	unset -nocomplain payload
} -result {match bar}
# -match glob -returnCodes error -errorCode {AWS NoSuchKey *} -result {AWS: The specified key does not exist.}
#>>>
test s3sigv4-1.3 {get key, all 0x1f < codepoints <= 0xff and \u306f} -constraints have_s3_bucket -setup { #<<<
	set key	{}
	for {set c 0x20} {$c <= 0xff} {incr c} {
		append key [format %c $c]
	}
	append key \u306f
} -body {
	aws s3 get_object -region $bucket_region -bucket $bucket -key [join [regexp -all -inline {.{80}} $key] /]
} -cleanup {
	unset -nocomplain key c
} -match glob -returnCodes error -errorCode {AWS NoSuchKey *} -result {AWS: The specified key does not exist.}
#>>>
#test s3sigv4-2.1 {list objects} -body { #<<<
#	expr {
#		[json length [aws s3 list_objects -region us-east-1 -bucket cyan-test-uploads -max_keys 4] Contents] > 0
#	}
#} -result 1
#>>>

set legacy_shim {
	namespace eval ::s3 { # API shim for old hand-written s3 module, on top of aws::s3 2
		namespace export *
		namespace ensemble create -prefixes no
		proc get args { #<<<
			parse_args::parse_args $args {
				-bucket				{-required}
				-response_headers	{-alias}
				-region				{-default ""}
				path				{-required}
			}

			if {$region eq ""} {set region us-east-1}

			set path    [join [lmap e [split $path /] {
				string map {/ %2f} [reuri decode $e]
			}] /]

			set res	[aws s3 get_object -region $region -bucket $bucket -key $path -payload bytes]
			set hdrs	{}
			json foreach {k v} $res {
				dict set hdrs [string tolower $k] [list [json get $v]]
			}
			set response_headers	$hdrs
			set bytes
		}

		#>>>
		proc upload args { #<<<
			parse_args::parse_args $args {
				-region				{-default ""}
				-bucket				{-required}
				-path				{-required}
				-data				{-required}
				-content_type		{}
				-max_age			{-default 31536000}
				-acl				{-enum public-read}
				-response_headers	{-alias}
			}

			if {![info exists content_type]} {
				package require Pixel 3.4.3
				try {
					set content_type	[pixel::image_mimetype $data]
				} trap {PIXEL CORE UNKNOWN_FILETYPE} {errmsg options} {
					set content_type	application/octet-stream
				}
			}

			switch -glob -- $content_type {
				text/* -
				application/json -
				application/javascript {
					set data	[encoding convertto utf-8 $data]
				}
			}

			if {$region eq ""} {set region us-east-1}

			if 0 {
			set res	[aws s3 put_object \
				-region			$region \
				-bucket			$bucket \
				-key			$path \
				-content_type	$content_type \
				-cache_control	max-age=$max_age \
				-ACL			$acl \
				-body			$data \
			]

			set hdrs	{}
			json foreach {k v} $res {
				dict set hdrs [string tolower $k] [list [json get $v]]
			}
			set response_headers $hdrs
			} else {
				set headers		[list Cache-Control max-age=$max_age]
				if {[info exists acl]} {
					lappend headers x-amz-acl $acl
				}
				# Hack the upload until the put_object generated proc is working
				# S3 doesn't like + in path elements
				set path	[join [lmap e [split $path /] {
					reuri encode path [reuri decode [string map {+ %2B} $e]]
				}] /]
				::aws::helpers::_aws_req PUT $bucket.s3.$region.amazonaws.com /$path \
					-region						$region \
					-scheme						https \
					-credential_scope			$region \
					-disable_double_encoding	1 \
					-signing_region				$region \
					-expecting_status			200 \
					-headers					$headers \
					-params						{} \
					-content_type				$content_type \
					-body						$data \
					-response_headers			response_headers \
					-status						status \
					-sig_service				s3 \
					-version					s3v4
			}

			return
		}

		#>>>
		proc exists args { #<<<
			parse_args::parse_args $args {
				-region				{-default ""}
				-bucket				{-required}
				-response_headers	{-alias}
				path				{-required}
			}

			if {$region eq ""} {set region us-east-1}
			try {
				# Hack the exists until the head_object generated proc is working
				# S3 doesn't like + in path elements
				set path	[reuri normalize $path]
				set path	[join [lmap e [split $path /] {
					string map {+ %2B} $e
				}] /]
				::aws::helpers::_aws_req HEAD $bucket.s3.$region.amazonaws.com /$path \
					-region						$region \
					-scheme						https \
					-credential_scope			$region \
					-disable_double_encoding	1 \
					-signing_region				$region \
					-expecting_status			200 \
					-response_headers			response_headers \
					-status						status \
					-sig_service				s3 \
					-version					s3v4
			} on ok {} {
				return 1
			} trap {AWS 404} {} - trap {AWS 403} {} {
				return 0
			}
		}

		#>>>
		proc delete args { #<<<
			parse_args::parse_args $args {
				-region		{-default ""}
				-bucket		{-required}
				-path		{-required}
			}

			if {$region eq ""} {set region us-east-1}

			aws s3 delete_object -region $region -bucket $bucket -key $path
		}

		#>>>
		proc ls args { #<<<
			parse_args::parse_args $args {
				-region				{-default {}}
				-prefix				{}
				-bucket				{-required}
				-delimiter			{}
				-max_keys			{-# {Defaults to 1000}}
				-continuation_token	{-default {}}
				-fetch_owner		{-boolean}
				-start_after		{}
				-encoding_type		{-enum url -# {If set to "url", responses are urlencoded (to permit C0 characters)i}}
			}

			set extra	{}
			if {$fetch_owner} {
				lappend extra -fetch_owner
			}
			if {[info exists start_after]} {
				lappend extra -start_after $start_after
			}
			if {[info exists encoding_type]} {
				lappend extra -encoding_type $encoding_type
			}
			if {$continuation_token ne {}} {
				lappend extra -continuation_token $continuation_token
			}
			if {[info exists max_keys]} {
				lappend extra -max_keys $max_keys
			}
			if {[info exists delimiter]} {
				lappend extra -delimiter $delimiter
			}
			if {[info exists prefix]} {
				lappend extra -prefix $prefix
			}
			if {$region eq {}} {
				set region us-east-1
			}

			set resp	[aws s3 list_objects_v2 \
				-region				$region \
				-bucket				$bucket \
				{*}$extra \
			]

			set truncated				[json get $resp IsTruncated]
			set next_continuation_token	[json get -default {} $resp NextContinuationToken]
			set res	[json template {
				{
					"truncated":				"~B:truncated",
					"next_continuation_token":	"~S:next_continuation_token",
					"results":					[]
				}
			}]

			if {[info exists delimiter]} {
				json set res commonprefixes {[]}
			}

			json foreach e [json extract $resp Contents] {
				json set res results end+1 [json template {
					{
						"key":			"~S:Key",
						"mtime":		"~S:LastModified",
						"etag":			"~S:ETag",
						"size":			"~N:Size",
						"storageclass":	"~S:StorageClass"
					}
				} [json get $e]]

				if {$fetch_owner} {
					json set res results end owner [json template {
						{
							"id":			"~S:ID",
							"displayname":	"~S:DisplayName"
						}
					} [json get $e Owner]]
				}
			}

			if {[json exists $resp CommonPrefixes]} {
				json foreach e [json extract $resp CommonPrefixes] {
					json set res commonprefixes end+1 [json get $e Prefix]
				}
			}

			set res
		}

		#>>>
		proc copy args { #<<<
			parse_args::parse_args $args {
				-bucket			{-required}
				-path			{-required}
				-region			{-default ""}
				-source_bucket	{-# {Defaults to -bucket}}
				-source			{-required}
				-max_age		{-default 31536000}
				-acl			{}
			}

			if {![info exists source_bucket]} {
				set source_bucket	$bucket
			}

			if {$region eq ""} {set region us-east-1}
			set extra	{}
			if {[info exists acl]} {
				lappend extra	-ACL $acl
			}

			aws s3 copy_object \
				-region			$region \
				-bucket			$bucket \
				-key			$path \
				-copy_source	$source_bucket/$source \
				-payload		payload \
				{*}$extra

			set payload
		}

		#>>>
	}
}

test s3sigv4-3.1 {path encoding: +} -constraints have_s3_bucket -setup { #<<<
	eval $legacy_shim
} -body {
	set key			foo+bar
	s3 upload -region $bucket_region -bucket $bucket -path $key -data $key -content_type text/plain
	set objs		[aws s3 list_objects_v2 -region $bucket_region -bucket $bucket -prefix $key]
	set get_resp	[aws s3 get_object -region $bucket_region -bucket $bucket -key $key -payload payload]
	aws s3 delete_object -region $bucket_region -bucket $bucket -key $key
	list [json get $objs Contents 0 Key] [json get $get_resp ContentLength] $payload
} -cleanup {
	namespace delete ::s3
	unset -nocomplain objs get_resp payload del_resp key
} -result {foo+bar 7 foo+bar}
#>>>
test s3sigv4-3.2 {path encoding: space} -constraints have_s3_bucket -setup { #<<<
	eval $legacy_shim
} -body {
	set key			{foo bar}
	s3 upload -region $bucket_region -bucket $bucket -path $key -data $key -content_type text/plain
	set objs		[aws s3 list_objects_v2 -region $bucket_region -bucket $bucket -prefix foo]
	set get_resp	[aws s3 get_object -region $bucket_region -bucket $bucket -key $key -payload payload]
	aws s3 delete_object -region $bucket_region -bucket $bucket -key $key
	list [json get $objs Contents 0 Key] [json get $get_resp ContentLength] $payload
} -cleanup {
	namespace delete ::s3
	unset -nocomplain objs get_resp payload del_resp key
} -result {{foo bar} 7 {foo bar}}
#>>>
test s3sigv4-3.3 {path encoding: /} -constraints {have_s3_bucket knownBug} -setup { #<<<
	eval $legacy_shim
} -body {
	set key			foo%2fbar
	s3 upload -region $bucket_region -bucket $bucket -path $key -data $key -content_type text/plain
	set objs		[aws s3 list_objects_v2 -region $bucket_region -bucket $bucket -prefix foo]
	set get_resp	[aws s3 get_object -region $bucket_region -bucket $bucket -key $key -payload payload]
	aws s3 delete_object -region $bucket_region -bucket $bucket -key $key
	list [json get $objs Contents 0 Key] [json get $get_resp ContentLength] $payload
} -cleanup {
	namespace delete ::s3
	unset -nocomplain objs get_resp payload del_resp key
} -result {foo%2Fbar 11 foo%2Fbar}
#>>>
test s3sigv4-3.4 {path encoding: :} -constraints have_s3_bucket -setup { #<<<
	eval $legacy_shim
} -body {
	set key			foo:bar
	s3 upload -region $bucket_region -bucket $bucket -path $key -data $key -content_type text/plain
	set objs		[aws s3 list_objects_v2 -region $bucket_region -bucket $bucket -prefix foo]
	set get_resp	[aws s3 get_object -region $bucket_region -bucket $bucket -key $key -payload payload]
	aws s3 delete_object -region $bucket_region -bucket $bucket -key $key
	list [json get $objs Contents 0 Key] [json get $get_resp ContentLength] $payload
} -cleanup {
	namespace delete ::s3
	unset -nocomplain objs get_resp payload del_resp key
} -result {foo:bar 7 foo:bar}
#>>>
test s3sigv4-3.5 {path encoding: ?} -constraints have_s3_bucket -setup { #<<<
	eval $legacy_shim
} -body {
	set key			foo?bar
	s3 upload -region $bucket_region -bucket $bucket -path $key -data $key -content_type text/plain
	set objs		[aws s3 list_objects_v2 -region $bucket_region -bucket $bucket -prefix foo]
	set get_resp	[aws s3 get_object -region $bucket_region -bucket $bucket -key $key -payload payload]
	aws s3 delete_object -region $bucket_region -bucket $bucket -key $key
	list [json get $objs Contents 0 Key] [json get $get_resp ContentLength] $payload
} -cleanup {
	namespace delete ::s3
	unset -nocomplain objs get_resp payload del_resp key
} -result {foo?bar 7 foo?bar}
#>>>
test s3sigv4-3.6 {path encoding: \u306f} -constraints have_s3_bucket -setup { #<<<
	eval $legacy_shim
} -body {
	set key			foo\u306fbar
	s3 upload -region $bucket_region -bucket $bucket -path $key -data $key -content_type {text/plain; charset=utf-8}
	set objs		[aws s3 list_objects_v2 -region $bucket_region -bucket $bucket -prefix foo]
	set get_resp	[aws s3 get_object -region $bucket_region -bucket $bucket -key $key -payload payload]
	aws s3 delete_object -region $bucket_region -bucket $bucket -key $key
	list [json get $objs Contents 0 Key] [json get $get_resp ContentLength] $payload [string length $payload]
} -cleanup {
	namespace delete ::s3
	unset -nocomplain objs get_resp payload del_resp key
} -result [list foo\u306fbar [string length [encoding convertto utf-8 foo\u306fbar]] foo\u306fbar 7]
#>>>

test s3_copy-1.1 {copy object} -constraints have_s3_bucket -setup { #<<<
	set key	s3_copy-1.1
	eval $legacy_shim
} -body {
	s3 upload -region $bucket_region -bucket $bucket -path $key -data s3_copy-1.1-data -content_type text/plain
	set res	[s3 copy -region $bucket_region -bucket $bucket -path $key-copy -source $key]
	list [s3 get -region $bucket_region -bucket $bucket $key-copy] [json exists $res ETag] [json exists $res LastModified]
} -cleanup {
	s3 delete -region $bucket_region -bucket $bucket -path $key
	s3 delete -region $bucket_region -bucket $bucket -path $key-copy
	unset -nocomplain key res
} -result {s3_copy-1.1-data 1 1}
#>>>

#set aws::debug 1
#try {
#	puts stderr "bucket location: [aws s3 get_bucket_location -bucket cyan-archive]"
#} on error {errmsg options} {
#	puts stderr [dict get $options -errorinfo]
#}

# cleanup
::tcltest::cleanupTests
return

# vim: ft=tcl foldmethod=marker foldmarker=<<<,>>> ts=4 shiftwidth=4
